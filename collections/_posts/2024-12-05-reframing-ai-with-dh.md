---
author: amanda-visconti
layout: post
date: 2024-12-05
title: "Reframing AI with the Digital Humanities"
slug: reframing-ai-with-dh
summary: "We already have the scholarly tools to critically analyze the ethics of new technology."
category: essay
tags:
- labor
- code
- research-and-development 
- ai-ml-llms
crosspost:
  - title: Amanda's blog
    url: https://literaturegeek.com/2024-12-05-reframing-ai-with-dh
---

*A version of this piece will be an open-access chapter in a volume by invited speakers at the 10/23/2024 ["Reimagining AI for Environmental Justice and Creativity"](https://karshinstitute.virginia.edu/events/reimagining-ai-environmental-justice-and-creativity) conference, co-organized by Jess Reia, MC Forelle, and Yingchong Wang and co-sponsored by UVA's Digital Technology for Democracy Lab, Environmental Institute, and School of Data Science. I had more to say, but this was what I managed inside the word limit!*

I direct [the Scholars' Lab](https://scholarslab.org), a digital humanities (DH) center that's led and collaborated on University of Virginia [ethical](https://scholarslab.lib.virginia.edu/charter/), creative experimentation at the intersections of humanities, culture, and tech since 2006. A common definition of DH encompasses both using digital methods (such as coding and mapping) to explore humanities research questions (such as concerns of history, culture, and art); and asking humanities-fueled questions about technology (such as ethical design review of tools like specific instances of AI). I always add a third core feature of DH: a set of socially just values and community practices around labor, credit, design, collaboration, inclusion, and scholarly communication, inseparable from best-practice DH.

I write this piece as someone with expertise in applicable DH subareas—research programming, digital scholarly design, and the ethical review of digital tools and interfaces—but not as someone with particular experience related to ML, LLMs, or other "AI" knowledges. A field of new and rapidly evolving tools means true expertise in the capabilities and design of AI is rare; often we are either talking about secondhand experiences of these tools (e.g. "Microsoft Co-Pilot let me xyz") or about AI as a shorthand for desired computing capabilities, unfounded on familiarity with current research papers or understanding of codebases. (A values-neutral claim: science fiction authors without technical skillsets have helped us imagine, and later create.) 

Convergence on the term "data science" has both inspired new kinds of work, and elided contributions of the significantly overlapping field of library and information studies. Similarly, "AI" as the shorthand for the last few years' significant steps forward in ML (and LLMs in particular) obscures the work of the digital humanities and related critical digital research and design fields such as Science and Technology Studies (STS). When we use the term "AI", it's tempting to frame our conversations as around a Wholly New Thing, focusing on longer-term technical aspirations uninhibited by practical considerations of direct audience needs, community impacts, resources. While that's not necessarily a bad way to fuel technological creativity, it's too often the only way popular conversations around AI proceed. In [one research blog post](https://thefrailestthing.com/2014/11/29/do-artifacts-have-ethics) exploring the moral and emotional dimensions of technological design, L.M. Sacasas lists 41 questions we can ask when designing technologies, from "What sort of person will the use of this technology make of me?" to "Can I be held responsible for the actions which this technology empowers? Would I feel better if I couldn’t?" We don't need to reinvent digital design ethics for AI—we've already got the approaches we need (though those can always be improved). 

When we frame "AI" as code, as a set of work discrete but continuous with a long history of programming and its packagings (codebase, repo, library, plugin…), it's easier to remember we have years of experience designing and analyzing the ethics and societal impacts of code—so much so that I've started assuming people who say "LLM" or "ML" rather than "AI" when starting conversations are more likely to be conversant with the specifics of current AI tech, as well as its ethical implications. The terms we use for our work and scholarly conversations are strategic: matching the language of current funding opportunities, job ads. We've seen similar technologically-vague popularizing on terms with past convergences of tech interest too, including MOOCs, "big data", and the move from "humanities computing" to the more mainstreamed "digital humanities".

Digital humanities centers like our Scholars' Lab offer decades of careful, critical work evaluating existing tools, contributing to open-source libraries, and coding and designing technology in-house—all founded on humanities skills related to history, ethics, narrative, and more strengths necessary to generative critique and design of beneficial tech. Some of the more interesting LLM-fueled DH work I've seen in the past couple years has involved an AI first- or second-pass at a task, followed by verification by humans—for situations where the verification step is neither more onerous nor more error-prone than a human-only workflow. For example:
* the Marshall Project had humans pull out interesting text from policies banning books in state prisons, [used AI](https://generative-ai-newsroom.com/decoding-bureaucracy-5b0c1411171) to generate useful summaries of these, then had humans check those summaries for accuracy
* Scholars [Ryan Cordell](https://bsky.app/profile/ryancordell.bsky.social/post/3kfvflcxg2k2t) and [Sarah Bull](https://bsky.app/profile/sarahebull.bsky.social/post/3kg2vhh45tm2y) tested Chat GPT's utility in classifying genres of historical newspaper and literary text from dirty OCR and without training data, and in OCR cleanup, with promising results
* My Scholars' Lab colleague Shane Lin has been exploring AI applications for OCRing text not well-supported by current tools, such as writing in right-to-left scripts
* Archaeologists restoring the HMS Victory [applied an AI-based algorithm](https://www.nmrn.org.uk/news/archaeology-meets-ai-ground-breaking-collaboration-between-national-museum-royal-navy-and) to match very high-resolution, high-detailed images stored in different locations to areas of a 3D model of the ship

One of DH's strengths has been its focus on shared methods and tools across disciplines, regardless of differences in content and disciplinary priorities, with practitioners regularly attending interdisciplinary conferences (especially unusual within the humanities) and discussing overlapping applications of tools across research fields. DH experts also prioritize non-content-agnostic conversations, prompted by the frequency with which we borrow and build on tools created for non-academic uses. For example, past Scholars' Lab DH Fellow Ethan Reed found utility in adapting a sentiment analysis tool from outside his field to exploring the emotions in Black Arts Poetry works, but also spent [a significant portion of his research writing](https://scholarslab.lib.virginia.edu/people/ethan-reed) critiquing the biased results based on the different language of sentiment in the tool's Rotten Tomatoes training dataset. (ML training sets are an easy locus for black boxing biases, context, and creator and laborer credit—similar to known issues with text digitization work, as explored by Aliza Elkin's troublingly gorgeous, free [Hand Job zine series](http://alizaelk.in/digitize) capturing Google Books scans that accidentally caught the often non-white, female or non-gender-conforming hands of the hidden people doing the digitizing.)

We already know where to focus to produce more beneficial, less harmful, creative digital tools: social justice. At the Reimagining AI roundtable, my table's consensus was that issues of power and bias are key not just to reducing ML harms, but to imagining and harnessing positive potential. Key areas of concern included climate terrorism (e.g. reducing the energy costs of data centers), racism (e.g. disproportionate negative impacts on BIPoC compounding existing economic, labor, and police violence threats), human rights (e.g. provision of a universal basic income easing concerns about areas ML may beneficially offset human labor), and intertwined ableist and computing access issues (e.g. AI search-result "slop" is terrible for screen readers, low-bandwidth internet browsing). In our existing scholarly fields and advocacy goals, where are current gaps in terms of abilities, resources, scale, efficiencies, audiences, ethics, and impacts? After identifying those major needs, we're better positioned to explore how LLMs might do good or ill.